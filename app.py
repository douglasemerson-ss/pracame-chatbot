import streamlit as st
import time
from langchain_chroma.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# -------------------------
# Streamlit config
# -------------------------
st.set_page_config(page_title="Pra√ßame Chatbot", page_icon="üî∞", layout="wide")
st.title("üî∞ Pra√ßame - Suporte T√©cnico Militar")
st.header("Este chatbot foi desenvolvido a partir da necessidade da equipe de TI para diminuir o fluxo de abertura de chamados.")
st.subheader("Atualmente sou uma vers√£o de testes ‚Äî respondo d√∫vidas sobre **Assinador SERPRO**.")

# -------------------------
# CSS / estilos
# -------------------------
st.markdown("""
<style>
.chat-container {
    max-width: 900px;
    margin-left: auto;
    margin-right: auto;
    padding-bottom: 90px;
    height: 70vh;
    overflow-y: auto;
    scroll-behavior: smooth;
}
.user-msg {
    background: #d9e6ff;
    color: #000;
    padding: 12px 16px;
    border-radius: 14px;
    margin: 6px 0;
    width: fit-content;
    max-width: 75%;
    word-wrap: break-word;
}
.bot-msg {
    background: #eef5e8;
    color: #000;
    padding: 12px 16px;
    border-radius: 14px;
    margin: 6px 0;
    width: fit-content;
    max-width: 75%;
    word-wrap: break-word;
}
.msg-row {
    display: flex;
    align-items: flex-start;
    margin-bottom: 10px;
}
.msg-row.user {
    justify-content: flex-end;
}
.avatar {
    width: 36px;
    height: 36px;
    border-radius: 50%;
    margin: 0 8px;
}
.typing {
    font-style: italic;
    color: #666;
}
 .scroll-fix { 
height: 10px; 
}
</style>
""", unsafe_allow_html=True)

# .scroll-fix { 
#height: 10px; 
#}


# -------------------------
# Session state
# -------------------------
if "historico" not in st.session_state:
    st.session_state["historico"] = []  # cada item: {"user": "...", "bot": "..."}

if "digitando" not in st.session_state:
    st.session_state["digitando"] = False

# -------------------------
# Load OpenAI key from Streamlit secrets
# -------------------------
# (Coloque OPENAI_API_KEY no Streamlit Secrets)
OPENAI_KEY = st.secrets.get("OPENAI_API_KEY", None)
if not OPENAI_KEY:
    st.error("OPENAI_API_KEY n√£o encontrada em Streamlit Secrets. V√° em Manage App ‚Üí Secrets e adicione.")
    st.stop()

CAMINHO_DB = "db"

# -------------------------
# Prompt seguro (n√£o repetir hist√≥rico)
# -------------------------
# Observa√ß√£o: n√≥s passamos o hist√≥rico ao prompt em formato *resumido* e damos instru√ß√µes claras
# para N√ÉO repetir as marca√ß√µes do hist√≥rico no texto de sa√≠da.
prompt_template = """
INSTRU√á√ïES IMPORTANTES (LIMITE R√çGIDO):
- Voc√™ s√≥ pode responder usando EXCLUSIVAMENTE a "Base de conhecimento" fornecida abaixo.
- N√ÉO invente, N√ÉO adivinhe e N√ÉO use conhecimento externo.
- N√ÉO repita literalmente as marca√ß√µes do hist√≥rico (por exemplo: "Usu√°rio:", "Assistente:") na sua resposta.

Base de conhecimento (trechos recuperados):
{base_conhecimento}

Hist√≥rico resumido (apenas para contexto, N√ÉO repita marca√ß√µes):
{historico}

Pergunta:
{pergunta}

Resposta (seja did√°tico, explique causas e passos de solu√ß√£o com linguagem simples)
"""

# -------------------------
# Carregar modelos e DB
# -------------------------
@st.cache_resource
def carregar_modelos():
    # Embeddings: definimos explicitamente o modelo do embedding (ajuste se desejar)
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY, model="text-embedding-3-small")
    db = Chroma(persist_directory=CAMINHO_DB, embedding_function=embeddings)
    # Chat model: escolha um modelo dispon√≠vel; gpt-4o-mini √© sugerido como default
    modelo = ChatOpenAI(openai_api_key=OPENAI_KEY, model="gpt-4o-mini", temperature=0.2)
    return embeddings, db, modelo

embeddings, db, modelo = carregar_modelos()

# -------------------------
# Container do chat
# -------------------------
chat_box = st.container()

def render_chat(scroll=False):
    """Renderiza todo o hist√≥rico e o indicador 'digitando'."""
    with chat_box:
        st.markdown('<div id="chatbox" class="chat-container">', unsafe_allow_html=True)

        for troca in st.session_state["historico"]:
            # user message
            st.markdown(
                f"""
                <div class="msg-row user">
                    <div class="user-msg">{troca['user']}</div>
                    <img class="avatar" src="https://cdn-icons-png.flaticon.com/512/9977/9977334.png">
                </div>
                """,
                unsafe_allow_html=True
            )

            # bot message (pode ser None ainda)
            if troca.get("bot"):
                st.markdown(
                    f"""
                    <div class="msg-row">
                        <img class="avatar" src="https://cdn-icons-png.flaticon.com/512/7985/7985432.png">
                        <div class="bot-msg">{troca['bot']}</div>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

        # indicador "digitando..."
        if st.session_state["digitando"]:
            st.markdown(
                f"""
                <div class="msg-row">
                    <img class="avatar" src="https://cdn-icons-png.flaticon.com/512/7985/7985432.png">
                    <div class="bot-msg typing">Digitando...</div>
                </div>
                """,
                unsafe_allow_html=True
            )

        st.markdown('</div>', unsafe_allow_html=True)

# S√≥ injeta o JS de scroll quando explicitamente pedido (scroll=True)
    if scroll:
        st.markdown("""
        <script>
            const box = document.getElementById("chatbox");
            if (box) {
                // pequenas pausas ajudam o browser a estabilizar o layout antes de rolar
                setTimeout(() => { box.scrollTop = box.scrollHeight; }, 120);
            }
        </script>
        """, unsafe_allow_html=True)

# render inicial
# render inicial (sem scroll autom√°tico)
render_chat(scroll=False)

render_chat()
st.markdown('',unsafe_allow_html=True)

# -------------------------
# Input do usu√°rio
# -------------------------
pergunta = st.chat_input("Digite sua d√∫vida...")

if pergunta:
    # 1) adicionar mensagem do usu√°rio imediatamente (sem resposta)
    st.session_state["historico"].append({"user": pergunta, "bot": None})

    # 2) ativar o indicador de digita√ß√£o e re-renderizar para o usu√°rio ver "Digitando..."
    st.session_state["digitando"] = True
    render_chat()
    # For√ßar scroll at√© o final (mostra a mensagem do usu√°rio e o "Digitando...")
    #st.markdown("""
    #<script>
        #var box = document.getElementById("chatbox");
        #if (box) { box.scrollTo({ top: box.scrollHeight, behavior: 'smooth' }); }
    #</script>
    #""", unsafe_allow_html=True)

    # 3) Agora geramos a resposta (bloqueante) ‚Äî mantenha isso abaixo para garantir que o UX mostre "Digitando..."
    ultima_msg = pergunta

    # recuperar vetores/fragmentos
    vetor = embeddings.embed_query(ultima_msg)
    resultados = db.similarity_search_by_vector_with_relevance_scores(vetor, k=6)  # k maior para seguran√ßa

    # Filtra resultados (opcional): aqui usamos TODOS e avaliamos no prompt
    if not resultados or len(resultados) == 0:
        # sem dados no √≠ndice
        resposta_final = "N√£o encontrei informa√ß√µes suficientes na base de conhecimento para responder a isso."
    else:
        # coletar conte√∫dos (voc√™ pode limitar aqui tamanho/quantidade)
        # mantemos a ordem original; juntamos os page_content
        textos_resultado = []
        for doc, score in resultados:
            textos_resultado.append(doc.page_content)

        base_conhecimento = "\n\n----\n\n".join(textos_resultado)

        # preparar hist√≥rico resumido - sem marca√ß√µes "Usu√°rio/Assistente"
        # vamos passar apenas as √∫ltimas N trocas (ex.: 6) para evitar prompt muito grande
        resumo_historico = []
        for troca in st.session_state["historico"][:-1]:  # sem a mensagem atual
            if troca.get("bot"):
                resumo_historico.append(f"User: {troca['user']}\nAssistant: {troca['bot']}")
            else:
                resumo_historico.append(f"User: {troca['user']}")

        # limitar tamanho do hist√≥rico a N √∫ltimas entradas
        resumo_historico_text = "\n\n".join(resumo_historico[-6:])

        # montar prompt com instru√ß√µes r√≠gidas
        prompt = ChatPromptTemplate.from_template(prompt_template)
        prompt_injetado = prompt.invoke({
            "historico": resumo_historico_text,
            "base_conhecimento": base_conhecimento,
            "pergunta": ultima_msg
        })

        # gerar resposta a partir do modelo
        # Este √© o ponto cr√≠tico ‚Äî o prompt instrui fortemente para n√£o "inventar"
        resposta_final = modelo.invoke(prompt_injetado).content

        # Se o modelo tentar burlar (por exemplo, responder algo muito curto ou gen√©rico),
        # voc√™ pode checar aqui e for√ßar a resposta padr√£o. Exemplo:
        if not resposta_final or len(resposta_final.strip()) < 10:
            resposta_final = "N√£o encontrei informa√ß√µes suficientes na base de conhecimento para responder a isso."

    # 4) salvar resposta no hist√≥rico
    st.session_state["historico"][-1]["bot"] = resposta_final

    # 5) desativar indicador digitando e re-renderizar tudo com a resposta
    st.session_state["digitando"] = False
    st.rerun(scroll=True)

    
    # 6) scroll suave para o final para garantir que o usu√°rio veja a resposta
    #st.markdown("""
    #<script>
        #var box = document.getElementById("chatbox");
        #if (box) { box.scrollTo({ top: box.scrollHeight, behavior: 'smooth' }); }
    #</script>
    #""", unsafe_allow_html=True)
