import streamlit as st
from langchain_chroma.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# -------------------------
#  CONFIGURA√á√ïES STREAMLIT
# -------------------------
st.set_page_config(page_title="Pra√ßame Chatbot", page_icon="üî∞")
st.header("üî∞ Pra√ßame - Suporte T√©cnico Militar")
st.write("Estou em vers√£o de testes, respondo d√∫vidas sobre problemas de hardware.")

# -------------------------
#  CARREGAR OPENAI API KEY
# -------------------------
OPENAI_KEY = st.secrets["OPENAI_API_KEY"]

CAMINHO_DB = "db"

prompt_template = """
Voc√™ √© um assistente t√©cnico militar especializado em suporte ao usu√°rio.
Todos os usu√°rios s√£o leigos no assunto, imagine que s√£o crian√ßas lidando com problemas de T.I.

Hist√≥rico da conversa at√© agora:
{historico}

Base de conhecimento relevante da documenta√ß√£o:
{base_conhecimento}

Pergunta atual do usu√°rio:
{pergunta}

Explique a causa do problema e ofere√ßa solu√ß√µes de forma super did√°tica, calma,
clara e com um linguajar simples.
"""

# -------------------------
#  ESTADO DA SESS√ÉO
# -------------------------
if "historico" not in st.session_state:
    st.session_state["historico"] = []

# -------------------------
#  CARREGAR MODELO + DB
# -------------------------
@st.cache_resource
def carregar_modelos():
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)
    db = Chroma(persist_directory=CAMINHO_DB, embedding_function=embeddings)
    modelo = ChatOpenAI(openai_api_key=OPENAI_KEY)
    return embeddings, db, modelo

embeddings, db, modelo = carregar_modelos()

# -------------------------
#  CAMPO DE INPUT
# -------------------------
pergunta = st.chat_input("Digite sua d√∫vida...")

if pergunta:

    st.session_state["historico"].append({"user": pergunta, "bot": None})

    # ---- BUSCAR NO BANCO DE VETORES ----
    vetor = embeddings.embed_query(pergunta)
    resultados = db.similarity_search_by_vector_with_relevance_scores(vetor, k=4)

    textos_resultado = [r[0].page_content for r in resultados]
    base_conhecimento = "\n\n----\n\n".join(textos_resultado)

    # ---- HIST√ìRICO FORMATADO ----
    historico_formatado = ""
    for troca in st.session_state["historico"]:
        if troca["bot"]:
            historico_formatado += f"Usu√°rio: {troca['user']}\nAssistente: {troca['bot']}\n"

    # ---- GERAR RESPOSTA ----
    prompt = ChatPromptTemplate.from_template(prompt_template)
    prompt_injetado = prompt.invoke({
        "historico": historico_formatado,
        "base_conhecimento": base_conhecimento,
        "pergunta": pergunta
    })

    resposta = modelo.invoke(prompt_injetado).content

    # salvar no hist√≥rico
    st.session_state["historico"][-1]["bot"] = resposta

# -------------------------
#  MOSTRAR MENSAGENS
# -------------------------
for troca in st.session_state["historico"]:
    with st.chat_message("user"):
        st.write(troca["user"])
    if troca["bot"]:
        with st.chat_message("assistant"):
            st.write(troca["bot"])
