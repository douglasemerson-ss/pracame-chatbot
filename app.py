import streamlit as st
from langchain_chroma.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

CAMINHO_DB = "db"

prompt_template = """
Voc√™ √© um assistente t√©cnico militar especializado em suporte ao usu√°rio.
Todos os usu√°rios s√£o leigos no assunto, imagine que s√£o crian√ßas lidando com problemas de T.I.

Hist√≥rico da conversa:
{historico}

Base de conhecimento relevante:
{base_conhecimento}

Pergunta atual do usu√°rio:
{pergunta}

Explique a causa do problema e ofere√ßa solu√ß√µes de forma super did√°tica, calma,
clara e com um linguajar simples.
"""

# ---- CONFIGURA√á√ÉO STREAMLIT ----
st.set_page_config(page_title="Pra√ßame Chatbot", page_icon="üî∞")
st.header("üî∞ Pra√ßame - Suporte T√©cnico Militar")
st.write("Estou em vers√£o de testes, apenas respondo algumas perguntas sobre Hardware")

# Inicializar sess√£o
if "historico" not in st.session_state:
    st.session_state["historico"] = []

# carregar modelo e base
@st.cache_resource
def carregar_modelos():
    api_key = st.secrets["OPENAI_API_KEY"]

    embeddings = OpenAIEmbeddings(api_key=api_key)
    db = Chroma(persist_directory=CAMINHO_DB, embedding_function=embeddings)

    modelo = ChatOpenAI(
        api_key=api_key,
        model="gpt-4o-mini",   # seguro + barato + r√°pido
        temperature=0.4
    )

    return embeddings, db, modelo

embeddings, db, modelo = carregar_modelos()

# Campo de input
pergunta = st.chat_input("Digite sua d√∫vida...")

if pergunta:
    # adicionar pergunta ao chat
    st.session_state["historico"].append({"user": pergunta, "bot": None})

    # buscar informa√ß√µes relevantes
    vetor = embeddings.embed_query(pergunta)
    resultados = db.similarity_search_by_vector_with_relevance_scores(vetor, k=4)
    textos_resultado = [r[0].page_content for r in resultados]
    base_conhecimento = "\n\n----\n\n".join(textos_resultado)

    # montar hist√≥rico
    historico_formatado = ""
    for troca in st.session_state["historico"]:
        if troca["bot"] is not None:
            historico_formatado += f"Usu√°rio: {troca['user']}\nAssistente: {troca['bot']}\n"

    # gerar resposta
    prompt = ChatPromptTemplate.from_template(prompt_template)
    prompt_injetado = prompt.invoke({
        "historico": historico_formatado,
        "base_conhecimento": base_conhecimento,
        "pergunta": pergunta
    })

    resposta = modelo.invoke(prompt_injetado).content

    # salvar e exibir
    st.session_state["historico"][-1]["bot"] = resposta

# mostrar hist√≥rico no chat
for troca in st.session_state["historico"]:
    with st.chat_message("user"):
        st.write(troca["user"])
    if troca["bot"]:
        with st.chat_message("assistant"):
            st.write(troca["bot"])
